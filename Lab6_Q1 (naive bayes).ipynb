{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab6 Q1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDdXa9Rw2y1P",
        "outputId": "c54c0484-0463-49c2-840a-f2713490f01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "                                    URL     Class\n",
            "0                       simply-loved-it  Positive\n",
            "1  most-disgusting-food-i-have-ever-had  Negative\n",
            "2        stay-away-very-disgusting-food  Negative\n",
            "3   menu-is-absolutely-perfect-loved-it  Positive\n",
            "4         a-really-good-value-for-money  Positive\n",
            "5        this-is-a-very-good-restaurant  Positive\n",
            "6                   terrible-experience  Negative\n",
            "7              this-place-has-best-food  Positive\n",
            "8  this-place-has-pathetic-food-serving  Negative\n",
            "\n",
            "Prior Probabilities: {'Positive': 0.5555555555555556, 'Negative': 0.4444444444444444}\n",
            "\n",
            "Vocab size: 32\n",
            "Total words in train data: 46\n",
            "\n",
            "Formed a dictionary of words with respect to their frequency and class\n",
            "\n",
            "\n",
            "Displaying all the conditional Probabilities:\n",
            "P(for/Positive) = 2/58            \tP(for/Negative) = 1/52            \t\n",
            "P(restaurant/Positive) = 2/58     \tP(restaurant/Negative) = 1/52     \t\n",
            "P(had/Positive) = 1/58            \tP(had/Negative) = 2/52            \t\n",
            "P(good/Positive) = 3/58           \tP(good/Negative) = 1/52           \t\n",
            "P(simply/Positive) = 2/58         \tP(simply/Negative) = 1/52         \t\n",
            "P(ever/Positive) = 1/58           \tP(ever/Negative) = 2/52           \t\n",
            "P(really/Positive) = 2/58         \tP(really/Negative) = 1/52         \t\n",
            "P(has/Positive) = 2/58            \tP(has/Negative) = 2/52            \t\n",
            "P(stay/Positive) = 1/58           \tP(stay/Negative) = 2/52           \t\n",
            "P(terrible/Positive) = 1/58       \tP(terrible/Negative) = 2/52       \t\n",
            "P(money/Positive) = 2/58          \tP(money/Negative) = 1/52          \t\n",
            "P(disgusting/Positive) = 1/58     \tP(disgusting/Negative) = 3/52     \t\n",
            "P(absolutely/Positive) = 2/58     \tP(absolutely/Negative) = 1/52     \t\n",
            "P(menu/Positive) = 2/58           \tP(menu/Negative) = 1/52           \t\n",
            "P(it/Positive) = 3/58             \tP(it/Negative) = 1/52             \t\n",
            "P(serving/Positive) = 1/58        \tP(serving/Negative) = 2/52        \t\n",
            "P(experience/Positive) = 1/58     \tP(experience/Negative) = 2/52     \t\n",
            "P(place/Positive) = 2/58          \tP(place/Negative) = 2/52          \t\n",
            "P(most/Positive) = 1/58           \tP(most/Negative) = 2/52           \t\n",
            "P(a/Positive) = 3/58              \tP(a/Negative) = 1/52              \t\n",
            "P(value/Positive) = 2/58          \tP(value/Negative) = 1/52          \t\n",
            "P(is/Positive) = 3/58             \tP(is/Negative) = 1/52             \t\n",
            "P(this/Positive) = 3/58           \tP(this/Negative) = 2/52           \t\n",
            "P(best/Positive) = 2/58           \tP(best/Negative) = 1/52           \t\n",
            "P(loved/Positive) = 3/58          \tP(loved/Negative) = 1/52          \t\n",
            "P(pathetic/Positive) = 1/58       \tP(pathetic/Negative) = 2/52       \t\n",
            "P(perfect/Positive) = 2/58        \tP(perfect/Negative) = 1/52        \t\n",
            "P(i/Positive) = 1/58              \tP(i/Negative) = 2/52              \t\n",
            "P(very/Positive) = 2/58           \tP(very/Negative) = 2/52           \t\n",
            "P(away/Positive) = 1/58           \tP(away/Negative) = 2/52           \t\n",
            "P(have/Positive) = 1/58           \tP(have/Negative) = 2/52           \t\n",
            "P(food/Positive) = 2/58           \tP(food/Negative) = 4/52           \t\n",
            "\n",
            "Displaying the result on test sentences:\n",
            "\n",
            "serving-good-food-abolutely-perfect-restaurant ===> Positive  [3.5024406e-10 1.7984046e-10]\n",
            "\n",
            "pathetic-food-ever-had ===> Negative  [9.8185083e-08 1.9451545e-06]\n",
            "\n",
            "Final Result:\n",
            "                                              URL     Class\n",
            "0  serving-good-food-abolutely-perfect-restaurant  Positive\n",
            "1                          pathetic-food-ever-had  Negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from enum import unique\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "all_words = []\n",
        "word_size_class = {}\n",
        "all_sents = []\n",
        "word_frequency_label = {}\n",
        "\n",
        "def load_data(filename):\n",
        "\n",
        "    data = pd.read_excel(filename)\n",
        "    test_index = data['Class'] == '?'\n",
        "    training_data = data[-test_index]\n",
        "    testing_data = data[test_index]   \n",
        "    testing_data.reset_index(inplace = True, drop = True) \n",
        "    \n",
        "    return (training_data,testing_data)\n",
        "\n",
        "def find_vocab_size(training_data):\n",
        "\n",
        "    for index,sent in enumerate(training_data['URL']):\n",
        "        ext_words = re.findall(r\"([a-z0-9]+)\",sent)\n",
        "        label = training_data['Class'][index]\n",
        "        word_size_class[label] = word_size_class.get(label,0) + len(ext_words)\n",
        "        all_words.extend(ext_words)\n",
        "        all_sents.append(ext_words)\n",
        "    \n",
        "    unique_words_count = len(set(all_words))\n",
        "    all_words_count = len(all_words)\n",
        "\n",
        "    global unique_words\n",
        "    unique_words = list(set(all_words))\n",
        "\n",
        "    return (unique_words_count,all_words_count)\n",
        "\n",
        "\n",
        "def find_prior_probabilties(training_data):\n",
        "    \n",
        "    class_prior = {}\n",
        "\n",
        "    labels = training_data['Class'].unique()\n",
        "    \n",
        "    total = len(training_data)\n",
        "    for l in labels:\n",
        "        class_prior[l] = sum(training_data['Class'] == l) / total\n",
        "\n",
        "    return class_prior\n",
        "\n",
        "\n",
        "def find_word_frequency_class(training_data):\n",
        "    \n",
        "    for word in unique_words:\n",
        "\n",
        "        for index,sent_vec in enumerate(all_sents):\n",
        "            if word in sent_vec:\n",
        "            \n",
        "                if word not in word_frequency_label:\n",
        "                    word_frequency_label[word] = {}\n",
        "                label = training_data['Class'][index]\n",
        "\n",
        "                word_frequency_label[word][label] = word_frequency_label[word].get(label,0) + sent_vec.count(word) \n",
        "\n",
        "\n",
        "def display_conditional_prob(vocab_size,labels):\n",
        "\n",
        "    i=0\n",
        "    for word in word_frequency_label:\n",
        "        \n",
        "        for label in labels:\n",
        "            num = word_frequency_label[word].get(label,0) + 1\n",
        "            denom = word_size_class[label] + vocab_size\n",
        "            space = \" \"\n",
        "            print(f\"P({word}/{label}) = {num}/{denom} {space*(14-len(word))}\",end=\"\\t\")\n",
        "\n",
        "        print()   \n",
        "   \n",
        "def display_test_results(data,labels,vocab_size,class_prior):\n",
        "    \n",
        "\n",
        "    for i,sent in enumerate(data['URL']):\n",
        "        ext_words = re.findall(r\"([a-z0-9]+)\",sent)\n",
        "        \n",
        "        probs = []\n",
        "        for label in labels:\n",
        "            prob = 1\n",
        "            for word in ext_words:\n",
        "                \n",
        "                class_dict = word_frequency_label.get(word)\n",
        "                \n",
        "                num = 0\n",
        "                denom = (word_size_class[label]+vocab_size)\n",
        "\n",
        "                if class_dict == None:\n",
        "                    num = 1\n",
        "                else:                    \n",
        "                    num = class_dict.get(label,0) + 1\n",
        "\n",
        "                #print(f\"{num}/{denom} ({word})  \",end=\" \")\n",
        "                prob *= (num/denom) \n",
        "            #print(f\"{label}\\n\")  \n",
        "\n",
        "            prior = class_prior[label]\n",
        "            probs.append(prior*prob)\n",
        "        \n",
        "        probs = np.array(probs, dtype=np.float32)\n",
        "\n",
        "        index = np.argmax(probs)\n",
        "        print(f\"\\n{sent} ===> {labels[index]}  {probs}\")\n",
        "        data['Class'][i] = labels[index]\n",
        "    \n",
        "    print(\"\\nFinal Result:\")\n",
        "    print(f\"{data}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    filename = \"./naive_bayes_data.xlsx\"\n",
        "    #filename = \"./test_data.xlsx\"\n",
        "\n",
        "    train_data,test_data = load_data(filename)\n",
        "\n",
        "    print(\"Training Data:\")\n",
        "    print(train_data)\n",
        "    \n",
        "    labels = train_data['Class'].unique()\n",
        "\n",
        "    class_prior = find_prior_probabilties(train_data)\n",
        "    print(f\"\\nPrior Probabilities: {class_prior}\\n\")\n",
        "\n",
        "    vocab_size, total_word_count = find_vocab_size(train_data)\n",
        "    print(f\"Vocab size: {vocab_size}\")\n",
        "    print(f\"Total words in train data: {total_word_count}\\n\")\n",
        "\n",
        "    find_word_frequency_class(train_data)\n",
        "    print(\"Formed a dictionary of words with respect to their frequency and class\\n\")\n",
        "\n",
        "    # for key,value in word_size_class.items():\n",
        "    #     #print(f\"No of words in '{key}' class: {value}\")\n",
        "    \n",
        "    print('\\nDisplaying all the conditional Probabilities:')\n",
        "    display_conditional_prob(vocab_size,labels)\n",
        "\n",
        "    print(\"\\nDisplaying the result on test sentences:\")\n",
        "    display_test_results(test_data,labels,vocab_size,class_prior)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "34Ic20dBZpKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "0f1d6d5e-6efd-4a3f-ae5d-804f3f969da3",
        "id": "amsVDDaHZs9b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-f706d7eb5f61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-f706d7eb5f61>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m#filename = \"./test_data.xlsx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Data:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-f706d7eb5f61>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'?'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'readcsv'"
          ]
        }
      ],
      "source": [
        "from enum import unique\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "\n",
        "all_words = []\n",
        "word_size_class = {}\n",
        "all_sents = []\n",
        "word_frequency_label = {}\n",
        "\n",
        "def load_data(filename):\n",
        "\n",
        "    data = pd.readcsv(filename)\n",
        "    test_index = data['Class'] == '?'\n",
        "    training_data = data[-test_index]\n",
        "    testing_data = data[test_index]   \n",
        "    testing_data.reset_index(inplace = True, drop = True) \n",
        "    \n",
        "    return (training_data,testing_data)\n",
        "\n",
        "# def find_vocab_size(training_data):\n",
        "\n",
        "#     for index,sent in enumerate(training_data['URL']):\n",
        "#         ext_words = re.findall(r\"([a-z0-9]+)\",sent)\n",
        "#         label = training_data['Class'][index]\n",
        "#         word_size_class[label] = word_size_class.get(label,0) + len(ext_words)\n",
        "#         all_words.extend(ext_words)\n",
        "#         all_sents.append(ext_words)\n",
        "    \n",
        "#     unique_words_count = len(set(all_words))\n",
        "#     all_words_count = len(all_words)\n",
        "\n",
        "#     global unique_words\n",
        "#     unique_words = list(set(all_words))\n",
        "\n",
        "#     return (unique_words_count,all_words_count)\n",
        "\n",
        "\n",
        "def find_prior_probabilties(training_data):\n",
        "    \n",
        "    class_prior = {}\n",
        "\n",
        "    labels = training_data['Class'].unique()\n",
        "    \n",
        "    total = len(training_data)\n",
        "    for l in labels:\n",
        "        class_prior[l] = sum(training_data['Class'] == l) / total\n",
        "\n",
        "    return class_prior\n",
        "\n",
        "\n",
        "# def find_word_frequency_class(training_data):\n",
        "    \n",
        "#     for word in unique_words:\n",
        "\n",
        "#         for index,sent_vec in enumerate(all_sents):\n",
        "#             if word in sent_vec:\n",
        "            \n",
        "#                 if word not in word_frequency_label:\n",
        "#                     word_frequency_label[word] = {}\n",
        "#                 label = training_data['Class'][index]\n",
        "\n",
        "#                 word_frequency_label[word][label] = word_frequency_label[word].get(label,0) + sent_vec.count(word) \n",
        "\n",
        "\n",
        "# def display_conditional_prob(vocab_size,labels):\n",
        "\n",
        "#     i=0\n",
        "#     for word in word_frequency_label:\n",
        "        \n",
        "#         for label in labels:\n",
        "#             num = word_frequency_label[word].get(label,0) + 1\n",
        "#             denom = word_size_class[label] + vocab_size\n",
        "#             space = \" \"\n",
        "#             print(f\"P({word}/{label}) = {num}/{denom} {space*(14-len(word))}\",end=\"\\t\")\n",
        "\n",
        "#         print()   \n",
        "   \n",
        "# def display_test_results(data,labels,vocab_size,class_prior):\n",
        "    \n",
        "\n",
        "#     for i,sent in enumerate(data['URL']):\n",
        "#         ext_words = re.findall(r\"([a-z0-9]+)\",sent)\n",
        "        \n",
        "#         probs = []\n",
        "#         for label in labels:\n",
        "#             prob = 1\n",
        "#             for word in ext_words:\n",
        "                \n",
        "#                 class_dict = word_frequency_label.get(word)\n",
        "                \n",
        "#                 num = 0\n",
        "#                 denom = (word_size_class[label]+vocab_size)\n",
        "\n",
        "#                 if class_dict == None:\n",
        "#                     num = 1\n",
        "#                 else:                    \n",
        "#                     num = class_dict.get(label,0) + 1\n",
        "\n",
        "#                 #print(f\"{num}/{denom} ({word})  \",end=\" \")\n",
        "#                 prob *= (num/denom) \n",
        "#             #print(f\"{label}\\n\")  \n",
        "\n",
        "#             prior = class_prior[label]\n",
        "#             probs.append(prior*prob)\n",
        "        \n",
        "#         probs = np.array(probs, dtype=np.float32)\n",
        "\n",
        "#         index = np.argmax(probs)\n",
        "#         print(f\"\\n{sent} ===> {labels[index]}  {probs}\")\n",
        "#         data['Class'][i] = labels[index]\n",
        "    \n",
        "#     print(\"\\nFinal Result:\")\n",
        "#     print(f\"{data}\\n\")\n",
        "\n",
        "def extraction(X,Y):\n",
        "\n",
        "  \n",
        "# Open file \n",
        "  with open('./naive_bayes_data - Copy.csv') as file_obj:\n",
        "      \n",
        "    # Create reader object by passing the file \n",
        "    # object to reader method\n",
        "    reader_obj = csv.reader(file_obj)\n",
        "      \n",
        "    # Iterate over each row in the csv \n",
        "    # file using reader object\n",
        "    for row in reader_obj:\n",
        "        print(row)\n",
        "\n",
        "  \n",
        "\n",
        "def main():\n",
        "\n",
        "    filename = \"./naive_bayes_data - Copy.csv\"\n",
        "    #filename = \"./test_data.xlsx\"\n",
        "\n",
        "    train_data,test_data = load_data(filename)\n",
        "\n",
        "    print(\"Training Data:\")\n",
        "    print(train_data)\n",
        "    \n",
        "    labels = train_data['Class'].unique()\n",
        "\n",
        "    #################################\n",
        "    converted = extraction(train_data,test_data)\n",
        "    print(converted)\n",
        "\n",
        "\n",
        "    class_prior = find_prior_probabilties(train_data)\n",
        "    print(f\"\\nPrior Probabilities: {class_prior}\\n\")\n",
        "\n",
        "    # vocab_size, total_word_count = find_vocab_size(train_data)\n",
        "    # print(f\"Vocab size: {vocab_size}\")\n",
        "    # print(f\"Total words in train data: {total_word_count}\\n\")\n",
        "\n",
        "    # find_word_frequency_class(train_data)\n",
        "    # print(\"Formed a dictionary of words with respect to their frequency and class\\n\")\n",
        "\n",
        "    # # for key,value in word_size_class.items():\n",
        "    # #     #print(f\"No of words in '{key}' class: {value}\")\n",
        "    \n",
        "    # print('\\nDisplaying all the conditional Probabilities:')\n",
        "    # display_conditional_prob(vocab_size,labels)\n",
        "\n",
        "    # print(\"\\nDisplaying the result on test sentences:\")\n",
        "    # display_test_results(test_data,labels,vocab_size,class_prior)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ]
}