{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "incomplete lab6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDUe7wRRlo8t",
        "outputId": "595ec5b5-5995-4d8d-d828-36398d1ed85e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' TDP TDP TDP TDP Sidhu Sidhu Sidhu BJP BJP BJP BJP BJP Sensex Congress Congress Congress Congress Congress Congress', 'Politics']\n",
            "[' Nifty Nifty Nifty Nifty Nifty BJP BJP Sensex Sensex Sensex Sensex Sensex Sensex Congress', 'Business']\n",
            "[' Sidhu Sidhu Sidhu Sidhu Sidhu Sidhu BJP Sixer Sixer Sixer Sixer Congress', 'Sports']\n",
            "[' TDP TDP TDP TDP Nifty BJP Sensex Congress Congress Congress Congress Congress Congress', 'Politics']\n",
            "[' Sixer Sixer Sixer Sixer Sixer Century Century Century Century Century Century', 'Sports']\n",
            "[' Nifty Nifty Nifty Nifty BJP BJP Sensex Sensex Sensex Sensex Sensex Sensex Century', 'Business']\n",
            "[' TDP TDP TDP TDP TDP BJP BJP BJP Congress Congress Congress Congress Congress', 'Politics']\n",
            "[' TDP TDP TDP TDP Sidhu Sidhu Sensex Congress Congress Congress Congress Congress Congress', '?']\n",
            "[' Sidhu Sidhu Sixer Sixer Sixer Sixer Sixer Sixer Sixer Sixer Sixer Century Century Century Century Century Century Century Century Century', '?']\n",
            "[' TDP TDP TDP TDP TDP Sidhu Sidhu BJP BJP BJP BJP BJP Sixer Sixer Sixer Sixer Sixer Sixer Sixer Sixer Sixer Century Century Century Century Century Century Century Century Century', '?']\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "def strings(row,count):\n",
        "  str1=\"\"\n",
        "  result_arr = ['Politics','Business','Sports','Politics','Sports','Business','Politics','?','?','?']\n",
        "  arr = []\n",
        "  fields = ['TDP','Nifty','Sidhu','BJP','Sensex','Sixer','Congress','Century']\n",
        "  for i in range(0,8):\n",
        "    temp = row[i];\n",
        "    #print(temp)\n",
        "    temp1 = int(temp)\n",
        "    for j in range(0,temp1):\n",
        "      str1 = str1 + \" \" + fields[i]  \n",
        "  arr.append(str1)\n",
        "  arr.append(result_arr[count])\n",
        "  return arr\n",
        "\n",
        "    \n",
        "\n",
        "filename = \"./naive_bayes_data - Copy.csv\"\n",
        "fields = []\n",
        "class_arr = ['Politics','Business','Sports','Politics','Sports','Business','Politics','?','?','?']\n",
        "rows = []\n",
        "mini_data = []\n",
        "with open(filename, 'r') as csvfile:\n",
        "    # creating a csv reader object\n",
        "    csvreader = csv.reader(csvfile)\n",
        "    fields = next(csvreader)\n",
        "    for k,row in enumerate(csvreader):\n",
        "        rows.append(row)\n",
        "        #print(row)\n",
        "        tempo = strings(row,k)\n",
        "        mini_data.append(tempo)\n",
        "for i in mini_data:\n",
        "  print(i)     "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "filename = \"sample.csv\"\n",
        "field=[\"URL\",\"Class\"]\n",
        "# writing to csv file \n",
        "with open(filename, 'w') as csvfile: \n",
        "    # creating a csv writer object \n",
        "    csvwriter = csv.writer(csvfile) \n",
        "        \n",
        "    # writing the fields \n",
        "    csvwriter.writerow(field) \n",
        "        \n",
        "    # writing the data rows \n",
        "    csvwriter.writerows(mini_data)"
      ],
      "metadata": {
        "id": "ndNTM0o9CDu7"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import unique\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "\n",
        "all_words = []\n",
        "word_size_class = {}\n",
        "all_sents = []\n",
        "word_frequency_label = {}\n",
        "\n",
        "def load_data(filename):\n",
        "\n",
        "    data = pd.read_csv(filename)\n",
        "    test_index = data['Class'] == '?'\n",
        "    training_data = data[-test_index]\n",
        "    testing_data = data[test_index]   \n",
        "    testing_data.reset_index(inplace = True, drop = True) \n",
        "    \n",
        "    return (training_data,testing_data)\n",
        "\n",
        "def find_vocab_size(training_data):\n",
        "\n",
        "    for index,sent in enumerate(training_data['URL']):\n",
        "        ext_words = re.findall(r\"([a-z0-9]+)\",sent)\n",
        "        label = training_data['Class'][index]\n",
        "        word_size_class[label] = word_size_class.get(label,0) + len(ext_words)\n",
        "        all_words.extend(ext_words)\n",
        "        all_sents.append(ext_words)\n",
        "    \n",
        "    unique_words_count = len(set(all_words))\n",
        "    all_words_count = len(all_words)\n",
        "\n",
        "    global unique_words\n",
        "    unique_words = list(set(all_words))\n",
        "\n",
        "    return (unique_words_count,all_words_count)\n",
        "\n",
        "\n",
        "def find_prior_probabilties(training_data):\n",
        "    \n",
        "    class_prior = {}\n",
        "\n",
        "    labels = training_data['Class'].unique()\n",
        "    \n",
        "    total = len(training_data)\n",
        "    for l in labels:\n",
        "        class_prior[l] = sum(training_data['Class'] == l) / total\n",
        "\n",
        "    return class_prior\n",
        "\n",
        "\n",
        "def find_word_frequency_class(training_data):\n",
        "    \n",
        "    for word in unique_words:\n",
        "\n",
        "        for index,sent_vec in enumerate(all_sents):\n",
        "            if word in sent_vec:\n",
        "            \n",
        "                if word not in word_frequency_label:\n",
        "                    word_frequency_label[word] = {}\n",
        "                label = training_data['Class'][index]\n",
        "\n",
        "                word_frequency_label[word][label] = word_frequency_label[word].get(label,0) + sent_vec.count(word) \n",
        "\n",
        "\n",
        "def display_conditional_prob(vocab_size,labels):\n",
        "\n",
        "    i=0\n",
        "    for word in word_frequency_label:\n",
        "        \n",
        "        for label in labels:\n",
        "            num = word_frequency_label[word].get(label,0) + 1\n",
        "            denom = word_size_class[label] + vocab_size\n",
        "            space = \" \"\n",
        "            print(f\"P({word}/{label}) = {num}/{denom} {space*(14-len(word))}\",end=\"\\t\")\n",
        "\n",
        "        print()   \n",
        "   \n",
        "def display_test_results(data,labels,vocab_size,class_prior):\n",
        "    \n",
        "\n",
        "    for i,sent in enumerate(data['URL']):\n",
        "        ext_words = re.findall(r\"([a-z0-9]+)\",sent)\n",
        "        \n",
        "        probs = []\n",
        "        for label in labels:\n",
        "            prob = 1\n",
        "            for word in ext_words:\n",
        "                \n",
        "                class_dict = word_frequency_label.get(word)\n",
        "                \n",
        "                num = 0\n",
        "                denom = (word_size_class[label]+vocab_size)\n",
        "\n",
        "                if class_dict == None:\n",
        "                    num = 1\n",
        "                else:                    \n",
        "                    num = class_dict.get(label,0) + 1\n",
        "\n",
        "                #print(f\"{num}/{denom} ({word})  \",end=\" \")\n",
        "                prob *= (num/denom) \n",
        "            #print(f\"{label}\\n\")  \n",
        "\n",
        "            prior = class_prior[label]\n",
        "            probs.append(prior*prob)\n",
        "        \n",
        "        probs = np.array(probs, dtype=np.float32)\n",
        "\n",
        "        index = np.argmax(probs)\n",
        "        print(f\"\\n{sent} ===> {labels[index]}  {probs}\")\n",
        "        data['Class'][i] = labels[index]\n",
        "    \n",
        "    print(\"\\nFinal Result:\")\n",
        "    print(f\"{data}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    #filename = \"./naive_bayes_data.xlsx\"\n",
        "    #filename = \"./test_data.xlsx\"\n",
        "    filename = \"./sample.csv\"\n",
        "\n",
        "    train_data,test_data = load_data(filename)\n",
        "\n",
        "    print(\"Training Data:\")\n",
        "    print(train_data)\n",
        "    \n",
        "    labels = train_data['Class'].unique()\n",
        "\n",
        "    class_prior = find_prior_probabilties(train_data)\n",
        "    print(f\"\\nPrior Probabilities: {class_prior}\\n\")\n",
        "\n",
        "    vocab_size, total_word_count = find_vocab_size(train_data)\n",
        "    print(f\"Vocab size: {vocab_size}\")\n",
        "    print(f\"Total words in train data: {total_word_count}\\n\")\n",
        "\n",
        "    find_word_frequency_class(train_data)\n",
        "    print(\"Formed a dictionary of words with respect to their frequency and class\\n\")\n",
        "\n",
        "    # for key,value in word_size_class.items():\n",
        "    #     #print(f\"No of words in '{key}' class: {value}\")\n",
        "    \n",
        "    print('\\nDisplaying all the conditional Probabilities:')\n",
        "    display_conditional_prob(vocab_size,labels)\n",
        "\n",
        "    print(\"\\nDisplaying the result on test sentences:\")\n",
        "    display_test_results(test_data,labels,vocab_size,class_prior)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISlWswh8E4A2",
        "outputId": "004ef47a-94b1-4f2c-a2bf-63aa4a10e8dc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "                                                 URL     Class\n",
            "0   TDP TDP TDP TDP Sidhu Sidhu Sidhu BJP BJP BJP...  Politics\n",
            "1   Nifty Nifty Nifty Nifty Nifty BJP BJP Sensex ...  Business\n",
            "2   Sidhu Sidhu Sidhu Sidhu Sidhu Sidhu BJP Sixer...    Sports\n",
            "3   TDP TDP TDP TDP Nifty BJP Sensex Congress Con...  Politics\n",
            "4   Sixer Sixer Sixer Sixer Sixer Century Century...    Sports\n",
            "5   Nifty Nifty Nifty Nifty BJP BJP Sensex Sensex...  Business\n",
            "6   TDP TDP TDP TDP TDP BJP BJP BJP Congress Cong...  Politics\n",
            "\n",
            "Prior Probabilities: {'Politics': 0.42857142857142855, 'Business': 0.2857142857142857, 'Sports': 0.2857142857142857}\n",
            "\n",
            "Vocab size: 6\n",
            "Total words in train data: 68\n",
            "\n",
            "Formed a dictionary of words with respect to their frequency and class\n",
            "\n",
            "\n",
            "Displaying all the conditional Probabilities:\n",
            "P(ongress/Politics) = 18/29        \tP(ongress/Business) = 2/29        \tP(ongress/Sports) = 2/28        \t\n",
            "P(entury/Politics) = 1/29         \tP(entury/Business) = 2/29         \tP(entury/Sports) = 7/28         \t\n",
            "P(ensex/Politics) = 3/29          \tP(ensex/Business) = 13/29          \tP(ensex/Sports) = 1/28          \t\n",
            "P(ifty/Politics) = 2/29           \tP(ifty/Business) = 10/29           \tP(ifty/Sports) = 1/28           \t\n",
            "P(ixer/Politics) = 1/29           \tP(ixer/Business) = 1/29           \tP(ixer/Sports) = 10/28           \t\n",
            "P(idhu/Politics) = 4/29           \tP(idhu/Business) = 1/29           \tP(idhu/Sports) = 7/28           \t\n",
            "\n",
            "Displaying the result on test sentences:\n",
            "\n",
            " TDP TDP TDP TDP Sidhu Sidhu Sensex Congress Congress Congress Congress Congress Congress ===> Politics  [4.8230027e-05 1.6386014e-11 8.4700455e-11]\n",
            "\n",
            " Sidhu Sidhu Sixer Sixer Sixer Sixer Sixer Sixer Sixer Sixer Sixer Century Century Century Century Century Century Century Century Century ===> Sports  [3.8742106e-29 8.2649829e-28 6.4394648e-12]\n",
            "\n",
            " TDP TDP TDP TDP TDP Sidhu Sidhu BJP BJP BJP BJP BJP Sixer Sixer Sixer Sixer Sixer Sixer Sixer Sixer Sixer Century Century Century Century Century Century Century Century Century ===> Sports  [3.8742106e-29 8.2649829e-28 6.4394648e-12]\n",
            "\n",
            "Final Result:\n",
            "                                                 URL     Class\n",
            "0   TDP TDP TDP TDP Sidhu Sidhu Sensex Congress C...  Politics\n",
            "1   Sidhu Sidhu Sixer Sixer Sixer Sixer Sixer Six...    Sports\n",
            "2   TDP TDP TDP TDP TDP Sidhu Sidhu BJP BJP BJP B...    Sports\n",
            "\n"
          ]
        }
      ]
    }
  ]
}